---
title: "TIMES-SERIES-PROJECT"
author: "BOA THIEMELE JEAN-LUC"
date: "2024-08-12"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# I-SEASONAL MODEL 
                      
## Installing libraries and packages
```{r installation}
#Installing Library
#install.packages('imputeTS')
#install.packages('forecast')
#install.packages("openxlsx")

#Loading Library
library('imputeTS')
library('readxl')
library('forecast')
library('ggplot2')
library('randomForest')
library('xgboost')
library('e1071')
library('openxlsx')
```

## Loading the data
```{r loading_dataset}
#Loading datasets
data <- read_excel("/Users/jlbt/Downloads/2023-11-Elec-train.xlsx")
colnames(data) <- c("times","power","temperature")
```

#Creation of times series object
```{r creation of time series}
#Transform data into a ts object
ts_data = ts(data,start=c(1,6), freq=(60*24)/15)
```

#Plotting the time series
```{r plotting_v1}
#Plot of the times series
plot(ts_data[,2], xlab="Time (15-minute intervals)", ylab="Electricity consumption", 
     main="Time Series Plot")
```
We can see that we have a span of observation that are 0, which goes from 2/18/2010 0:00 to 2/18/2010 2:30. We are going to apply an interpolation in order to have an estimated value for that span missing. But before we are going to split the data into train and validation.

#Let's create a train and test set
```{r train-test-part}
#let's create train and validation set and visualize t.hem
ts_train=window(ts_data[,2],start=c(1,6), end=c(40,96)) 
ts_test=window(ts_data[,2],start=c(41,1), end=c(51,96))

#Plotting of the test and train 
plot(ts_train, xlim=c(1,52))
lines(ts_test,lty=2,col=2)

```

```{r interpolation}
#Interpolation
ts_test[ts_test == 0] <- NA
ts_test=na_interpolation(ts_test)

#Plot of the result of the interpolation
plot(ts_test, xlab="Time (15-minute intervals)", ylab="Electricity consumption", main="Time Series Plot")
```
## Let's Analyse the train data

```{r plotting_v2}
ggtsdisplay(ts_train)
```
```{r decomposition}
power = decompose(ts_train)
autoplot(power)
```
Graph 1: The ACF graph indicates a strong seasonal pattern in the data.

Graph 2: This graph shows a decreasing trend in the data. Given these observations, our initial models for the dataset will include Simple Exponential Smoothing with additive seasonality and Exponential Smoothing with multiplicative seasonality. However, due to the frequency being 96, we cannot utilize the hw function to apply the damping effect. We will proceed with the HoltWinters() functions.

# I-1 - Holt winters with additive seasonal.
```{r holtwinters models 1}
#Let's define the model
LES_additive = HoltWinters(ts_train,alpha=NULL,beta=NULL,gamma=NULL, seasonal = "additive")

#Prediction with the model
p_additive<-predict(LES_additive,n.ahead=96*11)

#Let's plot the test set and the prediction
plot(ts_test,col=3)
lines(p_additive,col=2,lty=2)
legend("topleft", legend = c("test dataset", "Predictions of SES additive"), col = c("Green", "red"), lty = c(1,2),cex=0.6)

```
# I-2 - Holt winters with multiplicative seasonal.
```{r hw 2}

#Let's define the model
LES_multi=HoltWinters(ts_train, alpha=NULL, beta=NULL, gamma=NULL, seasonal = "multi")

#Prediction with the model
p_multi<-predict(LES_multi,n.ahead=96*11)

#Let's plot the test set and the prediction
plot(ts_test,col=3)
legend("topleft", legend = c("test dataset", "Predictions of SES multiplicative"), col = c("Green", "red"), lty = c(1,2),cex=0.6)
lines(p_multi,col=2,lty=2,)
```
## Let's compute RMSE for both models
```{r RMSE computation}
# Calculate RMSE for additive model
RMSE_additive <- sqrt(mean((p_additive - ts_test)^2))

# Calculate RMSE for multiplicative model
RMSE_multi <- sqrt(mean((p_multi - ts_test)^2))

# Save information into a DataFrame for analysis
results_df <- data.frame(
  Model = c(' SES Additive', 'SES Multiplicative'),
  RMSE = c(RMSE_additive, RMSE_multi)
)

print(results_df)

```
The results indicate that SES with multiplicative seasonality performs better than SES with additive seasonality. Given the strong seasonality present in the model, we will proceed to build a SARIMA model.

## II - STATIONNARY MODEL
```{r plotting-v3}
ggtsdisplay(ts_train)
```
There is a strong seasonal trend in the time series. We are going to differentiate with a lag=96
```{r plotting_v4}
ggtsdisplay(diff(ts_train,lag = 96,differences = 1))
```
There is a slight decreasing trend at the end of the time series. To eliminate this, we will apply an additional differencing step.

```{r plotting_v5}
ggtsdisplay(diff(diff(ts_train,lag = 96,differences = 1)))
```

```{r  analyzing resuslt}
power = decompose(diff(diff(ts_train,lag = 96,differences = 1)))
autoplot(power)
```
We succeded in suppressing all trend and seasonnal patterns in the times series, let's see if the autocorelation are white noise or not.
```{r box-test}
#let's perform Box-pierce test to see if we have white noise
Box.test(diff(diff(ts_train,lag = 96,differences = 1)),lag=10, type= 'Box-Pierce')
```
We conlude that the autocorelation are significant and we are not in presence of white noise.  There is some autocorrelations to modelize. Let's find a good model.

# II-1 - AUTO.ARIMA MODELING 
```{r auto arima}
# Let's apply auto arima and visualize the results
auto_arima=auto.arima(ts_train)
auto_arima %>% residuals() %>% ggtsdisplay()
```
```{r analyzing the model}
#Let's show the components of the model
auto_arima
```

The best model selected by auto.arima is ARIMA(1,0,0)(0,1,0)[96], with an AIC of 27838.62 and a BIC of 27851.07. Furthermore, the ar1 coefficients is significant, as it's estimated values are more than twice its standard errors. Let's proceed to check the residuals.

```{r plot}
checkresiduals(auto_arima,lag=12)
```
We can see that the pvalue < 2.2e-16 for lag=12. So the modelization does not capture all correlation in the data. Let's see results of prediction with that model.

```{r auto- arima}
prev_auto_arima = forecast(auto_arima,h=96*11)
autoplot(ts_test, series='ts_test')+autolayer(prev_auto_arima$mean,series="auto SARIMA")

```

```{r RMSE COMPUTATION auto arima}
# Calculate RMSE for auto arima
RMSE_auto_arima = sqrt(mean((prev_auto_arima$mean - ts_test)^2))
RMSE_auto_arima

```
The RMSE is 15.03 and so far the auto.arima is the best model to predict power. Let's try to found a better model manually.

# II-2 - MANNUAL ARIMA MODELING 
```{r ploting-v6}
ggtsdisplay(diff(diff(ts_train,lag = 96,differences = 1)))
```
Upon examining the PACF, we observe an exponential decrease at lags 96, 192, and 288, which correspond to the different periods of the dataset. This suggests that a seasonal ARIMA model may be appropriate.

In the ACF graph, there is a significant correlation at lag 96, which aligns with the period of the time series. Based on these observations, we can propose a SARIMA model.

For the seasonal part of the SARIMA model, we suggest using (0,1,1). For the non-seasonal ARMA part, the significant correlation at lag 96 indicates the characteristics of an MA(1) process, leading us to propose (0,1,1).

```{r model-arima-training}

#Let's define the model
fit_manual_arima=Arima(ts_train, order=c(0,1,1), seasonal=c(0,1,1))

#Let's analyze residuals
checkresiduals(fit_manual_arima,lag=20)

```
The Ljung-box test gives us a p-value < 2.2e-16. The residuals are significantly correlated, so the (0,1,1)(0,1,1) does not  capture all the correlation. We can see on the ACF that there is a significant correlation at lag=4. We will try a Sarima(0,1,4)(0,1,1)

```{r model-arima}

#Let's define the model
fit_manual_arima=Arima(ts_train, order=c(0,1,4), seasonal=c(0,1,1))

#Let's analyze residuals
checkresiduals(fit_manual_arima,lag=20)

```
 p-value < 7.117e-09. The residuals are still significantly correlated, so the (0,1,4)(0,1,1) does not capture all the correlation. we can see on the ACF that there is significant correlation at lag=10; we will try a sarima((0,1,10)(0,1,1)) 

```{r model-arima_2}

#Let's define the model
fit_manual_arima=Arima(ts_train, order=c(0,1,10), seasonal=c(0,1,1))

#Let's analyze residuals
checkresiduals(fit_manual_arima,lag=20)

```
We can see that the p-value = 0.26 , so we capture all correlation. We can use this model for forecasting.

```{r prediction and residuals}
#Prediction with the model
prev_manual_arima=forecast(fit_manual_arima,h=96*11)

#Ploting results
autoplot(ts_test)+autolayer(prev_manual_arima$mean,series=" auto SARIMA")

```

```{r analysing the manual arima model}
#Let's analyse de model
fit_manual_arima
```
All coefficients are significant except ma5,ma8 and ma9, the BIC is of 25684.5 which is lower than the one of the auto.arima. it is the same for the AIC. Let's compute RMSE for the manual ARIMA.
```{r RMSE comp}
# Evaluation of RMSE. for manual ARIMA
RMSE_manual_arima = sqrt(mean((prev_manual_arima$mean - ts_test)^2))
RMSE_manual_arima

```


The RMSE is lower than everything we had before, showing the good prediction given by the model. to explore further, we are going to test machine learning model to see if we can get better results.


# III - MACHINE LEARNING MODEL : RF,SVM,XGBOOST
```{r get the data ready}
# Convert the time series to a vector
ts_vector <- as.vector(ts_train)

# Create the lagged data matrix
n <- length(ts_vector)
window_size <- 97 #We are going to use the 96 last value to predict the 97 value.
data_ml <- matrix(nrow = n - window_size, ncol = window_size)

for (i in 1:(n - window_size)) {
  data_ml[i, ] <- ts_vector[i:(i + window_size - 1)]
}

```

```{r d}
# Prepare the data for the model
x_data <- data_ml[, -window_size]
y_data <- data_ml[, window_size]
```

# III-1 Random Forest
```{r b}
# Model RF
set.seed(123)
fitRF <- randomForest(x = x_data, y = y_data, mtry= 3, ntree=1000)

#RF
predRF <- numeric(96)
newdataRF <- tail(ts_vector, window_size-1)

for (t in 1:96) {
  predRF[t] <- predict(fitRF, newdata = matrix(newdataRF, nrow = 1, byrow = TRUE))
  newdataRF <- c(newdataRF[-1], predRF[t])
}

prevRF <- ts(predRF, start = c(41, 1), end = c(51, 96), frequency = 96)

plot(ts_test)
lines(prevRF,col=2,lty=2)
legend("topleft", legend = c("Test Data", "RF Predictions"), 
       col = c(1, 2), lty = c(1, 2), cex = 0.8)

cat('RMSE with RF :',sqrt(mean((ts_test-prevRF)^2)),'\n')

```
The Random forest gives an RMSE of 14.986 which is quite close of the one of manual arima we found in the previous section. We are going to test Support Vecteur Model (SVM) to see if it yield better result.

# III-2 SVM

```{r c}
# Model SVM
set.seed(123)
fitSVM=svm(x = x_data, y = y_data,kernel='radial',gamma=0.002)

#SVM
predSVM <- numeric(96)
newdataSVM <- tail(ts_vector, window_size-1)


for (t in 1:96) {
  predSVM[t]=predict(fitSVM,newdata=matrix(newdataSVM,1,96))
  newdataSVM=c(newdataSVM[-1],predSVM[t])
}

prevSVM <- ts(predSVM, start = c(41, 1), end = c(51, 96), frequency = 96)

plot(ts_test)
lines(prevSVM,col=2,lty=2)
legend("topleft", legend = c("Test Data", "SVM Predictions"), 
       col = c(1, 2), lty = c(1, 2), cex = 0.8)

cat('RMSE with SVM :',sqrt(mean((ts_test-prevSVM)^2)),'\n')

```
The RMSE is 15.74 with a gamma = 0.002 which is the best. Random Forest perform better than SVM. let's test XGBOOST

# III-3 - XGBOOST
```{r e}
# Model XGBOOST
set.seed(123)
model<- xgboost(data= as.matrix( x_data), label = y_data,
max_depth = 3, eta = 0.5, nrounds = 200,
nthread = 2, objective = "reg:squarederror", alpha=1)

#XGB
predXGB <- numeric(96)
newdataXGB <- as.matrix(tail(ts_vector, window_size-1))


for (t in 1:96) {
  predXGB[t] <- predict(model, newdata =  xgb.DMatrix(matrix(newdataXGB, nrow = 1, byrow = TRUE)))
  newdataXGB <- c(newdataXGB[-1], predXGB[t])
}

prevXGB <- ts(predXGB, start = c(41, 1), end = c(51, 96), frequency = 96)

plot(ts_test)
lines(prevXGB,col=2,lty=2)
legend("topleft", legend = c("Test Data", "XGB Predictions"), col = c(1, 2), lty = c(1, 2), cex = 0.8)

cat('RMSE with XGB :',sqrt(mean((ts_test-prevXGB)^2)),'\n')

```
The XGBOOST gives the worst result when comparing RMSE to the one of SVM  and RF, we got a RMSE of 16.42. None of the model are better than the manual arima that we found which has an RMSE of 14.79. we will then perform our final prediction with manual auto arima.
. Let's plot the result of the machine learning part
```{r f}
plot(ts_test)
lines(prevSVM,col=7, lty=2)
lines(prevXGB,col=5,lty=2)
lines(prevRF,col=2,lty=2)
legend("topleft", legend = c("Test Data", "SVM Predictions", "XGBoost Predictions", "RF Predictions"), 
       col = c(1, 7, 5, 2), lty = c(1, 2, 2, 2), cex = 0.8)
```

```{r g}
RMSE_RF = sqrt(mean((ts_test-prevRF)^2))
RMSE_SVM = sqrt(mean((ts_test-prevSVM)^2))
RMSE_XGB = sqrt(mean((ts_test-prevXGB)^2))

# Save information into a DataFrame for analysis
results_df <- data.frame(
  Model = c(' SES Additive', 'SES Multiplicative','auto.arima model','manual arima model','RF','SVM','XGBOOST' ),
  RMSE = c(RMSE_additive, RMSE_multi,RMSE_auto_arima, RMSE_manual_arima,RMSE_RF,RMSE_SVM,RMSE_XGB)
)
results_df <- results_df[order(results_df$RMSE),]

# Afficher le data.frame rangÃ©

print(results_df)


```
#lET'S MAKE A PREDICTION WITH ALL THE DATA
```{r h}
#Selecting all the power data without the day to predict ( which is empty)
all_data = data[1:4891,2:3]
colnames(all_data) <- c("power","temperature")

#Interpolation on all the data. 
all_data[all_data == 0] <- NA
all_data=na_interpolation(all_data)

#Make all_data a ts object
ts_data_all = ts(all_data,start=c(1,6), freq=(60*24)/15)

#Training of the model with the manual arima
fit_manual_arima=Arima(ts_data_all[,1], order=c(0,1,10), seasonal=c(0,1,1))

#Making the prediction
prev_manual_arima=forecast(fit_manual_arima,h=96)

#Let's see the result
autoplot(ts_data_all[,1])+autolayer(prev_manual_arima$mean,series=" auto SARIMA")

```

```{r i}
#Save it in xlsx file
forecast_results <- data.frame(forecast_without_using_outdoor_Temperature =
                                 as.numeric(prev_manual_arima$mean))
write.xlsx(forecast_results, "BOA-THIEMELE.xlsx")
```


# VI- PREDICTION WITH COVARIATE : OUTDOOR TEMPERATURE
```{r j}
#Let's redefine train and test set
ts_train_2=window(ts_data,start=c(1,6), end=c(40,96)) 

ts_test_2=window(ts_data,start=c(41,1), end=c(51,96))
ts_test_2[ts_test_2 == 0] <- NA
ts_test_2=na_interpolation(ts_test_2)
```

#  VI-1- AUTO-ARIMA with covariate
```{r klp}
#Applying auto Arima with covariate (xreg = temperature)
auto_arima_fit_covariate=auto.arima(ts_train_2[,"power"],xreg=ts_train_2[,"temperature"])
#Checking residuals
checkresiduals(auto_arima_fit_covariate,lag=20)
```
There is still some autocorrelations in the final modelization. we are going to see if we can do better with manual arima. but before let's analyse the model and make forecast with it

```{r lopj}
# Model analysis
auto_arima_fit_covariate
```

AIC is 27838.94 and all coefficient are significant, the arima chosen is the same as previous ARIMA(1,0,0)(0,1,0)[96] .Let's make prediction with the model
```{r m}
#Forecasting with the build model
prev_auto_arima_cov=forecast(auto_arima_fit_covariate,h=96*11,xreg=ts_test_2[,"temperature"])

#Ploting the results
autoplot(ts_test_2[,"power"])+autolayer(prev_auto_arima_cov$mean)

```

```{r n}
#Computing RMSE of predictions with covariate
RMSE_covariate = sqrt(mean((prev_auto_arima_cov$mean-ts_test_2[,"power"])^2))
RMSE_covariate
```



#  VI-2- MANUAL ARIMA WITH COVARIATE
```{r gy}
#Removing covariate effect
lstm_fit=tslm(power~temperature+trend+season ,data=ts_train_2)
summary(lstm_fit)

```
```{r ik}
lstm_fit_2 = tslm(power~temperature, data=ts_train_2)
summary(lstm_fit_2)
```
```{r qrt}
#Cross validation of the previous models
CV(lstm_fit_2)
CV(lstm_fit)

```
The second regression has the best BIC, so we will proceed with that one. we will use regression with season and trend.
```{r opoi}
#Looking at the residuals
checkresiduals(lstm_fit)
```
There is a trend and a seasonnal pattern, we will differentiate twice to suppress them as previously.
```{r ocxp}
tsdisplay(lstm_fit$residuals)
```
```{r udg}
# Differentiating the residuals
tmp=diff(diff(lstm_fit$residuals,lag = 96,differences = 1))
```

```{r zerl}
#Let's plot the result
ggtsdisplay(tmp)
```
We got similar result as previously with manual arima, we will try Sarima(0,1,10)(0,1,1)
```{r lop}
#Running the model and checking residuals
manual_arima_with_covariate_fit=Arima(lstm_fit$residuals,order=c(0,1,10), seasonal = c(0,1,1))
checkresiduals(manual_arima_with_covariate_fit,20)
```
P=0.251, we completely capture all correlation and we can use it on the base model with covariate.

```{r tym}
#Model with covariate
manual_arima_with_covariate_fit=Arima(ts_train_2[,"power"],xreg=ts_train_2[,"temperature"],
                                      order=c(0,1,10),seasonal = c(0,1,1))
checkresiduals(manual_arima_with_covariate_fit,lag=20)
```
The p value is 0.2646, we capture all correlation. We can make prediction with the model and compute RMSE.
```{r nhgd}
#Forecasting with the build model
prev_fit=forecast(manual_arima_with_covariate_fit,h=96*11,xreg=ts_test_2[,"temperature"])
                  
```

```{r oug}
#Ploting the results
autoplot(ts_test_2[,"power"])+autolayer(prev_fit$mean)
```

```{r uugp}
#Computing RMSE
RMSE_manual_arima_covariate = sqrt(mean((ts_test-prev_fit$mean)^2))
RMSE_manual_arima_covariate
```
the RMSE is lower than the one of the auto arima with covariate.

# VI-3 NEURAL NETWORK : NNETAR WITH COVARIATE
```{r SDS}
nnetar_fit = nnetar(ts_train_2[,"power"], xreg= ts_train_2[,"temperature"]) 
print(nnetar_fit)
```
```{r jhb}

prev_nnetar=forecast(nnetar_fit,xreg=tail(ts_train_2[,"temperature"],96*11))
autoplot(ts_test_2[,"power"])+autolayer(prev_nnetar$mean)

```
```{r hb}
RMSE_nnetar = sqrt(mean((ts_test-prev_nnetar$mean)^2))
RMSE_nnetar
```

The RMSE is really bad and we can see on the graph that the model doesn't fit well the data. We will then make the final prediction with the manual arima with covariate
```{r uycc}

#Training of the model with the manual arima
fit_manual_arima_covariate=Arima(ts_data_all[,1], order=c(0,1,10), seasonal=c(0,1,1),
                                 xreg=ts_data_all[,2])

#Making the prediction
prev_manual_arima_covariate=forecast(fit_manual_arima_covariate,h=96,xreg=tail(ts_data_all[,2],96))

#Let's see the result
autoplot(ts_data_all[,1])+autolayer(prev_manual_arima_covariate$mean,series="SARIMA")
```


```{r r}
# Load the workbook
wb <- loadWorkbook('BOA-THIEMELE.xlsx')

# Add a title in the first cell of the second column (B1)
writeData(wb, sheet = 1, x = "Manual_arima_with_temperature_as_covariate", startCol = 2, startRow = 1, colNames = FALSE)

# Write the data starting from the second row
writeData(wb, sheet = 1, x = prev_manual_arima_covariate$mean, startCol = 2, startRow = 2, colNames = FALSE)
prev_manual_arima_covariate
# Save the workbook
saveWorkbook(wb, 'BOA-THIEMELE.xlsx', overwrite = TRUE)

```


---
title: "EXAM ASML PART 2-Exercise 2 - question 2"
author: "Jean-Luc BOA THIEMELE"
date: "2024-01-15"
output: pdf_document
---

```{r}
# Install and load required packages
#install.packages("VSURF")
#install.packages("randomForest")
#install.packages("rpart") 
```

```{r}
library(rpart)
library("randomForest")
library(VSURF)
library(glmnet)
```

```{r}
# Load the PM10 dataset from the VSURF package.
data("jus")
```

```{r}
# Custom column names
jus = jus[,c("NO","NO2","SO2","T.min","T.max","T.moy","DV.maxvv","DV.dom"   
             ,"VV.max","VV.moy","PL.som","HR.min","HR.max"
             ,"HR.moy","PA.moy","GTrouen","GTlehavre","PM10")]
```

```{r}
# We supress the na from the dataset --> 64
jus <- na.omit(jus)
rownames(jus) <- NULL
```

```{r}
set.seed(123)
#Splitting data
n <- nrow(jus)
inTrain <- sample.int(n, size = 0.8 * n, replace = FALSE)
training <- jus[inTrain,]
test = jus[-inTrain,]
rownames(test) <- NULL
rownames(training) <- NULL
```
Function that construct the best tree after determining the maximal tree
```{r}
best_cart_model=function (T)
{
  table_of_cp = T$cptable
  cross_validation_error=table_of_cp[, 4] #We extract the cross validation error columns
  
  #We get the index of the minimum of the CV error
  index_min_cv = which(cross_validation_error==min(cross_validation_error))
  
  # We compute the threshold as min(cv_error) + its standard deviation 
  threshold = min(table_of_cp[index_min_cv, 4] + table_of_cp[index_min_cv, 5])
  
  #threshold=min(threshold) 
  #We extract the index of all the cross validation error less or equal to the threshold
  index_cv_inf_thres = which(cross_validation_error<=threshold)
  
  #Get the first index of the index of cv inferior to threshold => best cp 
  best_cp_index =index_cv_inf_thres[1] 
  # We prune the maximal tree by using the best cp value to get the best cart model
  Tf=prune(T, cp=table_of_cp[best_cp_index, 1]) 
  
  best_cart_model=Tf
}
```
Performing variables selection

```{r}
#We perform variable selection with VSURF
set.seed(123)
variable_selection = VSURF(PM10 ~.,data=training)
```
MODEL 1: Training of the model using the variable selected after the interpret step

```{r}
set.seed(123)
#The varaibles selected after the interpret step are : NO, NO2, GTrouen, 
#VV.moy, GTlehavre, SO2, T.max, T.moy, PL.som, VV.max, HR.moy, DV.maxvv
selection_interp_step = variable_selection$varselect.interp

#re-sepecifying the columns in the dataset
dataset_interp_step = training[,c(colnames(training[selection_interp_step]),"PM10")]

#We contruct the maximal tree using CART
maximal_tree_interpret_variables = rpart(PM10 ~ .,data=dataset_interp_step,minsplit=2,cp=10^(-9))

#We determine the best tree using the variables selected
model_interp = best_cart_model(maximal_tree_interpret_variables)
print(model_interp)
```
Model 2: Training of the model using the variable selected after the prediction step

```{r}
set.seed(123)
#The variables selected after the prediction step are : NO, GTrouen, VV.moy, GTlehavre, 
#SO2, T.max, PL.som
selection_pred_step = variable_selection$varselect.pred

#re-specifying the columns in the dataset
dataset_pred_step = training[,c(colnames(training[selection_pred_step]),"PM10")]
maximal_tree_predict_variables = rpart(PM10 ~ .,data=dataset_pred_step,minsplit=2,cp=10^(-9))

#We determine the best tree using the variables selected
model_predict = best_cart_model(maximal_tree_predict_variables)
print(model_predict)
```
MODEL 3 : Training of the model using CART algorithm

```{r}
set.seed(123)
# Construction of the maximal tree
max_tree = rpart(PM10 ~ .,data=training,minsplit=2,cp=10^(-9))

#Construction of the best
cart_model = best_cart_model(max_tree) 
print(cart_model)
```
MODEL 4 : Training of the model using Random Forest

```{r}
set.seed(123)
## Training of random forest
rf = randomForest(PM10 ~.,data=training)
rf
```

MODEL 5 : Training of the model using MCO

```{r}
set.seed(123)
#We perform a regression
lr = lm(PM10 ~.,data=training)
summary(lr)
#The only part we can interpret are the adjusted R2 (0.5494) and the coefficient estimate.
#Before analyzing the other part we need to know if the residuals follows N(0,constant)
#If not, we can't use the results of tests, the residual standard error 
#, the std of each estimate because they have all been calculated assuming
#the redisudal follows N(0,constant)

```

```{r}
set.seed(123)
#The residuals are not identically distributed, so we need 
#to perform some transformation so that they become idd.
#Computation of studendized residual which distribution is a student(N-3)
student_residual = rstudent(lr)

#Detect outliers
plot(lr$fitted.values,student_residual)
abline(h=2,col='red')
abline(h=-2,col='red')

#Get the index of the observation that are consider as outliers
a=which(student_residual>2) 
b=which(student_residual<(-2))
```

```{r}
#We are going to delete all the outliers
set.seed(123)
training_v2=training[-a,]
training_v2=training_v2[-b,]

lr_b=lm(PM10~.,data=training_v2)
student_residual_b = rstudent(lr_b) # studentized residuals

#we draw the qqplot for the residuals--> #We can see 
#that the residuals follow a normal distribution with mean 0 and constant standard deviation.
theoritical_quantile = qt(ppoints(797), df = 794)
qqplot(theoritical_quantile, student_residual_b) #student_residual are empirical quantile
qqline(student_residual_b,distribution=function(p){qt(p,df=794)},col='red',lty=2)

```

```{r}
#We perform a Kolmogorov-Smirnov test to have an analytic proof
#We obtain a big p-value, so we can not reject the null hypothesis which is : 
#we can not reject the fact that the residuals follows a student law with 794 degree of liberty
ks.test(student_residual_b,'pt',794) # --> p-value :0.1792

summary(lr_b)
```


```{r}
set.seed(123)
#Variables selection using glmnet
Las=glmnet(training_v2[,-18],training_v2[,18])

#we create a collection of models and we are going to select the best one using cross validation
sLas=cv.glmnet(as.matrix(training_v2[,-18]),training_v2[,18])
#We select lambda that is going to give us the best model
lambdaf=sLas$lambda.1se

#We construct the model with the value lambdaf : the variable selected are those wihtout a dot in s0.
Lasf=glmnet(training_v2[,-18],training_v2[,18],lambda=lambdaf)
print(Lasf$beta)
```


```{r}
# Subset the training_v2 dataset
new_training <- training_v2[, c("NO", "NO2", "SO2", 
                                "T.max", "DV.maxvv", "DV.dom","VV.max", 
                                "VV.moy", "PL.som", "HR.max", "HR.moy" ,"PA.moy",
                                "GTrouen", "GTlehavre", "PM10")]
```

```{r}
set.seed(123)
lr_final = lm(PM10 ~.,data=new_training )
```

```{r}
#Results of the final linear regression
summary(lr_final)
```

EVALUATION OF MODELS
```{r}
  eval=function(test,model_interp,model_predict,rf,cart_model,lr_final) # function to evaluate models
{
    
  # Prediction with each model
  pred_interp = predict(model_interp,newdata=test) 
  pred_prediction = predict(model_predict,newdata=test) #the same with the second model
  pred_rf = predict(rf,newdata=test)
  pred_cart = predict(cart_model,newdata=test)
  pred_lr_final = predict(lr_final,newdata=test)
  
  # Computing errors fpr each model
  testerr_interp=1/(nrow(test))*sum((test$PM10-pred_interp)^2)
  testerr_prediction=1/(nrow(test))*sum((test$PM10-pred_prediction)^2)
  testerr_rf=1/(nrow(test))*sum((test$PM10-pred_rf)^2)
  testerr_cart=1/(nrow(test))*sum((test$PM10-pred_cart)^2)
  testerr_lr_final=1/(nrow(test))*sum((test$PM10-pred_lr_final)^2)

  # Storing results in a list
  eval=list(testerr_interp, testerr_prediction, testerr_rf, testerr_cart,testerr_lr_final)
  
}
```

```{r}
set.seed(123)
results = eval(test, model_interp,model_predict,rf,cart_model,lr_final)
```

```{r}
results <- matrix(results, nrow = 5, ncol = 1)
row_names <- c("errors_interp","Erros_pred","Errors_rf","Errors_cart_model","errors_lr_final")
rownames(results) <- row_names
colnames(results) <- "model_errors"
print(results)
```

```{r}

"Random forest (Errors_rf) achieves the lowest error (26.06769 ) , 
indicating superior predictive performance compared to others models.
However, interpreting random forest results can be challenging
due to the complexity of the model. Random forests operate by constructing
multiple decision trees during training and outputting the mean prediction
of the individual trees for regression.
So, despite its superior performance, the Linear regression model
(errors_lr_final) might be preferred in practice due to their ease
of interpretation. We also notice that model_cart and the model 
with the variables selected after the prediction step retained
the same variable for the construction of the best tree and that's 
why they give the same errors values.
"
```

